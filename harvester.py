import requests
import pandas as pd
import time
from datetime import datetime
import os

print("üåç Big Data Harvester (–°—Ö—ñ–¥–Ω–∞ –Ñ–≤—Ä–æ–ø–∞) –∑–∞–ø—É—â–µ–Ω–æ!")
print("–°–∫–∞–Ω—É—î–º–æ –£–∫—Ä–∞—ó–Ω—É —Ç–∞ —Å—É—Å—ñ–¥—ñ–≤...")

API_TOKEN = "89bb553f1a69e42b0a03af6dd05c6b3a26aa2a70"
FILE_NAME = "real_ground_history.csv"
BOUNDS = "43.0,20.0,54.0,40.0"
BANNED_WORDS = ["Russia", "Belarus", "–†–§", "–ë–µ–ª–∞—Ä—É—Å—å", "Russian Federation"]

def fetch_and_save():
    new_data = []
    current_time = datetime.now().strftime('%Y-%m-%d %H:00:00')
    print(f"\n[{current_time}] üì° –ü–æ—à—É–∫ –¥–∞—Ç—á–∏–∫—ñ–≤ —É –∑–∞–¥–∞–Ω–æ–º—É –∫–≤–∞–¥—Ä–∞—Ç—ñ...")
    
    try:
        map_url = f"https://api.waqi.info/map/bounds/?latlng={BOUNDS}&token={API_TOKEN}"
        stations_req = requests.get(map_url, timeout=10).json()
        
        if stations_req.get('status') != 'ok':
            print("‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–∞—Ä—Ç–∏.")
            return

        stations = stations_req['data']
        print(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(stations)} –∞–∫—Ç–∏–≤–Ω–∏—Ö —Å—Ç–∞–Ω—Ü—ñ–π. –ü–æ—á–∏–Ω–∞—é –¥–µ—Ç–∞–ª—å–Ω–∏–π –∑–±—ñ—Ä...")

        for s in stations:
            uid = s['uid']
            name = s['station']['name']
            
            if any(banned.lower() in name.lower() for banned in BANNED_WORDS):
                continue
                
            try:
                details = requests.get(f"https://api.waqi.info/feed/@{uid}/?token={API_TOKEN}", timeout=5).json()
                
                if details.get('status') == 'ok':
                    iaqi = details['data'].get('iaqi', {})
                    pm25 = float(iaqi.get('pm25', {}).get('v', 0))
                    pm10 = float(iaqi.get('pm10', {}).get('v', 0))
                    no2 = float(iaqi.get('no2', {}).get('v', 0))
                    
                    if pm25 > 0 or pm10 > 0:
                        new_data.append({
                            'time': current_time,
                            'station': name,
                            'uid': uid,
                            'pm2.5': pm25,
                            'pm10': pm10,
                            'no2': no2
                        })
            except Exception as e:
                pass 
            
            time.sleep(0.1)

        if new_data:
            df_new = pd.DataFrame(new_data)
            
            # üåü –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–µ —Ç—ñ–ª—å–∫–∏ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É, –∞–ª–µ —ñ —á–∏ –≤—ñ–Ω –ù–ï –ø–æ—Ä–æ–∂–Ω—ñ–π (> 0 –±–∞–π—Ç)
            if os.path.exists(FILE_NAME) and os.path.getsize(FILE_NAME) > 0:
                try:
                    df_existing = pd.read_csv(FILE_NAME)
                    df_combined = pd.concat([df_existing, df_new]).drop_duplicates(subset=['time', 'uid'], keep='last')
                    df_combined.to_csv(FILE_NAME, index=False)
                except pd.errors.EmptyDataError:
                    # –Ø–∫—â–æ —Ñ–∞–π–ª —è–∫–æ—Å—å –ø–æ–ª–∞–º–∞–≤—Å—è, —Å—Ç–≤–æ—Ä—é—î–º–æ –ø–æ–≤–µ—Ä—Ö –Ω—å–æ–≥–æ –Ω–æ–≤–∏–π
                    df_new.to_csv(FILE_NAME, index=False)
            else:
                # –Ø–∫—â–æ —Ñ–∞–π–ª—É –Ω–µ–º–∞—î –∞–±–æ –≤—ñ–Ω –ø–æ—Ä–æ–∂–Ω—ñ–π (0 –±–∞–π—Ç)
                df_new.to_csv(FILE_NAME, index=False)
            
            print(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(new_data)} –∑–∞–ø–∏—Å—ñ–≤ —É Data Lake.")

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ —Ü–∏–∫–ª—É: {e}")

if __name__ == "__main__":
    fetch_and_save()
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ —Ü–∏–∫–ª—É: {e}")

if __name__ == "__main__":

    fetch_and_save()
